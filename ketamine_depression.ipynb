{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e06e2f3a-7106-45b3-aeab-c7b51ba5ea78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import mne,os,glob                    \n",
    "from mne.io import read_raw_ctf\n",
    "from pathlib import Path\n",
    "from sklearn import mixture\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score, normalized_mutual_info_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef0a7d6-ccd8-48eb-a0d0-c37d9359938d",
   "metadata": {},
   "source": [
    "# Globus_raw_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f34fd4-963c-4df9-8591-616cb11a266e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rootdir = '/media/ubuntu/HDD_P2/MDD/1sttrial/*'\n",
    "training_data = np.empty((0,271,3000))\n",
    "training_data1 = np.empty((0,271,3000))\n",
    "i=0\n",
    "for path in glob.glob(rootdir):\n",
    "    if path.endswith('.ds'):\n",
    "        i+=1\n",
    "        # read .ds file\n",
    "        raw = read_raw_ctf(path, preload=True)   \n",
    "        # resampling from 1200 to 300Hz \n",
    "        raw.resample(300).pick('meg').load_data()\n",
    "        # make 10s epochs from continuous data \n",
    "        duration = 10.\n",
    "        events = mne.make_fixed_length_events(raw, duration=duration)\n",
    "        tmax = duration - 1. / raw.info['sfreq']\n",
    "        # picks = mne.pick_types(raw.info, meg=True,ref_meg=False)\n",
    "        epochs = mne.Epochs(raw, events=events, tmin=0, tmax=tmax,baseline=None)\n",
    "\n",
    "        epoch1 = epochs.load_data()\n",
    "        # picking onlu meg channels\n",
    "        epoch2 = epochs.pick_types(meg=True, ref_meg=False).load_data()\n",
    "       # print(epochs.ch_names)\n",
    "        # dropping extra channels\n",
    "        if len(epochs.ch_names)==272:\n",
    "            epochs.drop_channels(['MLT31-1609'])\n",
    "        if len(epochs.ch_names)==273:\n",
    "            epochs.drop_channels(['MLT31-1609'])\n",
    "            epochs.drop_channels(['MLF25-1609'])\n",
    "        epoch4 = epochs.get_data()\n",
    "        print(np.shape(epoch4))\n",
    "        epochs.load_data().filter(l_freq=4, h_freq=13)\n",
    "        picks = mne.pick_types(raw.info, meg=True, ref_meg=False)\n",
    "        alpha_data = epochs.get_data()\n",
    "        #l1 = alpha_data.shape[0]\n",
    "        # concatenate unfiltered data \n",
    "        training_data=np.concatenate((training_data,epoch4[:,:,:]),axis=0)\n",
    "        # concatenate filtered data \n",
    "        training_data1=np.concatenate((training_data1,alpha_data[:,:,:]),axis=0)\n",
    "print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6a2504-6469-470d-a539-446fb1d5b3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving filtered and unfiltered data \n",
    "np.savez('/media/ubuntu/HDD_P2/MDD/globus_data2.npz', data=training_data, data1 = training_data1, labels = labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e50f095-0991-4c3b-8443-53165f87009b",
   "metadata": {},
   "source": [
    "# Bids_mdd sample data with labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8c5d17-8a26-4d11-bfb5-2ba4ad0772e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading labels for bids_mdd sample resting state datatest for supervised learning \n",
    "df = pd.read_excel('/home/ubuntu/Downloads/bids_lab.xlsx')\n",
    "print(df.head())\n",
    "df1 = df['labels'].values\n",
    "print(df1[0])\n",
    "print(len(df1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2fd05d-bdd4-4dff-b04a-e14747663d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading BIDS data \n",
    "blabels=[]\n",
    "lab=[]\n",
    "la=[]\n",
    "\n",
    "j=0\n",
    "rootdir = '/media/ubuntu/HDD_P2/MDD/DATA'\n",
    "btraining_data = np.empty((0,271,3000))\n",
    "btraining_data1 = np.empty((0,271,3000))\n",
    "for path in glob.glob(f'{rootdir}/*/'):\n",
    "    path1 = os.path.join(path,'meg/')\n",
    "#    print(path1)\n",
    "\n",
    "    for raw_path, i  in zip(glob.glob(f'{path1}/*'),range(44)):\n",
    "        if raw_path.endswith('_task-rest_run-1_meg.ds'):\n",
    "            # loading bids_data label\n",
    "            l = df1[i]\n",
    "            la.append(l)\n",
    "            j +=1\n",
    "            #read bids_sample data .ds file\n",
    "            raw = read_raw_ctf(raw_path, preload=True)   \n",
    "            # resampling from 1200 to 300Hz\n",
    "            raw.resample(300).load_data()\n",
    "            # epoching into 10s trial\n",
    "            duration = 10.\n",
    "            events = mne.make_fixed_length_events(raw, duration=duration)\n",
    "            tmax = duration - 1. / raw.info['sfreq']\n",
    "            picks = mne.pick_types(raw.info, meg=True,ref_meg=False)\n",
    "            epochs = mne.Epochs(raw, events=events, tmin=0, tmax=tmax,\n",
    "                                baseline=None)\n",
    "            sfreq = epochs.info['sfreq']\n",
    "            n_time_samps = raw.n_times\n",
    "            time_secs = raw.times\n",
    "            ch_names = raw.ch_names\n",
    "            n_chan = len(ch_names)  # note: there is no raw.n_channels attribute\n",
    "            print('the (cropped) sample data object has {} time samples and {} channels.'\n",
    "                  ''.format(n_time_samps, n_chan))\n",
    "            print('The last time sample is at {} seconds.'.format(time_secs[-1]))\n",
    "            print('The first few channel names are {}.'.format(', '.join(ch_names[:3])))\n",
    "            print()  # insert a blank line in the output\n",
    "\n",
    "            # some examples of raw.info:\n",
    "            print('bad channels:', raw.info['bads'])  # chs marked \"bad\" during acquisition\n",
    "            print(raw.info['sfreq'], 'Hz')            # sampling frequency\n",
    "            print(raw.info['description'], '\\n')      # miscellaneous acquisition info\n",
    "\n",
    "            print(raw.info)\n",
    "            \n",
    "            epoch1 = epochs.load_data()\n",
    "            #selecting meggrad channels\n",
    "            epoch3 = epochs.pick_types(meg=True, ref_meg=False).load_data()\n",
    "            #removing channel which is not in competition dataset \n",
    "            if len(epochs.ch_names)==272:\n",
    "                epochs.drop_channels(['MLT31-1609'])\n",
    "            if len(epochs.ch_names)==273:\n",
    "                epochs.drop_channels(['MLT31-1609'],['MLF25-1609'])\n",
    "            \n",
    "            epoch4 = epochs.get_data()\n",
    "            print(np.shape(epoch4))\n",
    "            #filtering data in alpha band \n",
    "            epochs.load_data().filter(l_freq=4, h_freq=13)\n",
    "            picks = mne.pick_types(raw.info, meg=True, ref_meg=False)\n",
    "            alpha_data = epochs.get_data()\n",
    "            #appending labels based on number of trials for each subject  # nonuniform trials\n",
    "            l1 = alpha_data.shape[0]\n",
    "            lab.append(l1)\n",
    "            for i in range(l1):\n",
    "                blabels.append(l)\n",
    "            #unfiltered data \n",
    "            btraining_data=np.concatenate((btraining_data,epoch4[:,:,:]),axis=0)\n",
    "            #filtered data\n",
    "            btraining_data1=np.concatenate((btraining_data1,alpha_data[:,:,:]),axis=0)\n",
    "        \n",
    "print('j=',j)\n",
    "\n",
    "#tf1 = pd.DataFrame(la).to_csv('/media/ubuntu/HDD_P2/la.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed4e406-2d16-4bcf-b150-a513c1cfafc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving bids_mdd filtered and unfiltered data \n",
    "np.savez('/media/ubuntu/HDD_P2/MDD/bids_data3.npz', data=btraining_data, data1 = btraining_data1, blabels = blabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd6a679-dc3a-4af4-b426-98adee5427af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading bids unfiltered data \n",
    "data = btraining_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcad04b-5646-4171-bded-8f4059da33b9",
   "metadata": {},
   "source": [
    "# splitting, covariance matrix , channel selection, Tangent space "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de08b1c2-10c5-4c4d-8451-5b108b7ed6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting bids_data into train and validation set using kfold \n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from pyriemann.estimation import Covariances\n",
    "from pyriemann.channelselection import ElectrodeSelection\n",
    "from pyriemann.tangentspace import TangentSpace\n",
    "kfold = KFold(n_splits=3, shuffle=True,random_state=10)\n",
    "## enumeration of the splits\n",
    "t=[];v=[];t1=[];v1=[];t2=[];v2=[];l1=[];l2=[]\n",
    "mat=[];channel=[]\n",
    "for (train, val) in kfold.split(data):\n",
    "    print('train: %s, val: %s' % (data[train].shape, data[val].shape))\n",
    "    print('trainl: %s, vall: %s' % (label[train].shape, label[val].shape))\n",
    "    l1.append(label[train]);l2.append(label[val])\n",
    "    \n",
    "    #Calculating pyriemann covariance matrix as feature \n",
    "    newmat = Covariances(estimator='lwf').fit(data[train], label[val])                                         \n",
    "    mat.append(newmat)\n",
    "\n",
    "    new_train = newmat.transform(data[train])\n",
    "    t.append(new_train)\n",
    "    cov_val = newmat.transform(data[val])\n",
    "    v.append(cov_val)\n",
    "    \n",
    "    #Feature selection\n",
    "    channel_train = ElectrodeSelection(nelec =24).fit(new_train, label[train], sample_weight=None)\n",
    "    channel.append(channel_train)\n",
    "   \n",
    "    \n",
    "    select_train = channel_train.transform(new_train)\n",
    "    t1.append(select_train)   \n",
    "    #Projecting train data on tangent space for feeding into model \n",
    "    select_t1 = TangentSpace().fit_transform(select_train)\n",
    "    t2.append(select_t1)\n",
    "   \n",
    "    #feature selection for validation data\n",
    "    select_val = channel_train.transform(cov_val)\n",
    "    v1.append(select_val)\n",
    "    #Projecting validation data on tangent space for feeding into model \n",
    "    select_v1 = TangentSpace().fit_transform(select_val)\n",
    "    v2.append(select_v1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20047fe4-0d66-4863-90cc-1547c56a0238",
   "metadata": {},
   "source": [
    "# Covarinace feature for globus test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac093ae6-58f9-48de-844b-6de4eebd113c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading competition test data (Globus)\n",
    "gdata = np.load('/media/ubuntu/HDD_P2/MDD/globus_data2.npz')\n",
    "g_data = gdata['data']\n",
    "\n",
    "#covatiance matrix, channel selection, tangent space transformation\n",
    "cov_test = mat[2].transform(g_data)\n",
    "print('shape of covariance matrix=', cov_test.shape)\n",
    "globus_test = channel_train.transform(cov_test)\n",
    "ts_test = TangentSpace().fit_transform(globus_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529afef3-44d8-44ad-8981-974d89367689",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('riema_test.npz', cov_test=cov_test, ts_test=ts_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93b0c12-293b-4892-87c2-73604fbd87c1",
   "metadata": {},
   "source": [
    "# kMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180fce2f-ce5e-47b7-a6c6-cd95cca4545a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "km = KMeans(n_clusters = 2, n_init = 40)\n",
    "km.fit(ts_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb84f366-b153-459a-ba21-14aaf27057d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "km.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b05854-3100-4236-a091-f45e1a4a425c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(km.labels_).to_csv('/media/ubuntu/HDD_P2/MDD/kmeans.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae51552-8b4b-4721-824d-6e2f71a74c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "m2 =[]\n",
    "for i in range(36):\n",
    "    l1 = km.labels_[((26*i)-(i-1))-1:(25*(i+1)+1)-1]\n",
    "    #print((range(((26*i)-(i-1))-1,(25*(i+1)+1)-1)))\n",
    "    #print(l.shape)\n",
    "    most_common1,num_most_common1 = Counter(l1).most_common(1)[0] # 4, 6 times\n",
    "    #print(most_common, num_most_common)\n",
    "    m2.append(most_common1)\n",
    "tf1 = pd.DataFrame(m2).to_csv('/media/ubuntu/HDD_P2/MDD/final2/kmeans.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43578945-de66-460c-b8b8-89c76be3da95",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(m2).count(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3078790-a373-4bd4-b8cd-6a753edf0b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "m2.count(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636cc5cc-37b4-4602-8fe3-39a4bba5ad65",
   "metadata": {},
   "source": [
    "# GMM predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1618de64-92cc-4122-a230-a27afcd64f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import mixture\n",
    "gmm = mixture.GaussianMixture(n_components=2).fit(ts_test)\n",
    "labels = gmm.predict(ts_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8164f20-2f27-49e9-a6e4-3ef342250f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(labels).to_csv('/media/ubuntu/HDD_P2/MDD/gmm_globus.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bf9fdd2c-028a-46c4-8bf0-63fe2dbb5616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   A  B\n",
      "0  0  1\n",
      "1  1  1\n",
      "2  2  1\n",
      "3  3  1\n",
      "4  4  1\n",
      "1\n",
      "900\n",
      "   Id  GMM\n",
      "0   1    0\n",
      "1   2    1\n",
      "2   3    0\n",
      "3   4    1\n",
      "4   5    1\n",
      "0\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/media/ubuntu/HDD_P2/MDD/gmm_globus.csv')\n",
    "print(df.head())\n",
    "df1 = df['B'].values\n",
    "print(df1[0])\n",
    "print(len(df1))\n",
    "from collections import Counter\n",
    "m2 =[]\n",
    "for i in range(36):\n",
    "    l1 = df1[((26*i)-(i-1))-1:(25*(i+1)+1)-1]\n",
    "    #print((range(((26*i)-(i-1))-1,(25*(i+1)+1)-1)))\n",
    "    #print(l.shape)\n",
    "    most_common1,num_most_common1 = Counter(l1).most_common(1)[0] # 4, 6 times\n",
    "    #print(most_common, num_most_common)\n",
    "    m2.append(most_common1)\n",
    "tf1 = pd.DataFrame(m2).to_csv('/media/ubuntu/HDD_P2/MDD/final2/gmm.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5c0dfa-75e1-4d6f-8f3e-ca87c58d75b1",
   "metadata": {},
   "source": [
    "# GMM + kMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bff2b0-ad25-46f6-b9b5-dfb5efe416be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Initialize KMeans and GaussianMixture models\n",
    "kmeans = KMeans(n_clusters=2, max_iter=1000,tol=1e-4)\n",
    "\n",
    "gm = GaussianMixture(n_components=2, max_iter=1000, tol=1e-4,init_params='random')\n",
    "\n",
    "# Fit and predict the algorithms\n",
    "y_kmeans = kmeans.fit_predict(test_data)\n",
    "y_gm = gm.fit_predict(test_data)\n",
    "y_gm_proba = gm.predict_proba(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe50491-06bc-4294-a54f-0ca88b7a4cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_gm_proba.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003c9a07-3d04-433b-883b-10e945c333b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gm = GaussianMixture(n_components=2, max_iter=3000, tol=1e-4,init_params='random')\n",
    "\n",
    "gm_kmeans = GaussianMixture(n_components=2,  max_iter=3000, tol=1e-4,init_params='kmeans')\n",
    "\n",
    "y_gm = gm.fit_predict(test_data)\n",
    "y_gm_proba = gm.predict_proba(test_data)\n",
    "\n",
    "y_gm_kmeans = gm_kmeans.fit_predict(test_data)\n",
    "y_gm_proba_kmeans = gm_kmeans.predict_proba(test_data)\n",
    "\n",
    "#plt.plot_different_gms(train_data, 0.34,  y_gm, y_gm_proba,  y_gm_kmeans, y_gm_proba_kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111a46c7-1b18-4c8d-8175-fae8b7669bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_gm_proba_kmeans[:,0].round()\n",
    "pred4 = [int(x) for x in y_gm_proba_kmeans[:,0].round()]\n",
    "np.shape(pred4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37ddd4c-2e4f-46f1-8490-dde3d807da19",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = pd.DataFrame(pred4).to_csv('/media/ubuntu/HDD_P2/MDD/final2/globus.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fe0540-938d-46a1-b6d9-03d108515c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(pred4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf171fdc-495b-4798-bae8-5b6c65d89d08",
   "metadata": {},
   "source": [
    "# counter globus test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede0bce1-289e-4c9f-9e65-fa6b4b9fc56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "m2 =[]\n",
    "for i in range(36):\n",
    "    l1 = pred4[((26*i)-(i-1))-1:(25*(i+1)+1)-1]\n",
    "    #print((range(((26*i)-(i-1))-1,(25*(i+1)+1)-1)))\n",
    "    #print(l.shape)\n",
    "    most_common1,num_most_common1 = Counter(l1).most_common(1)[0] # 4, 6 times\n",
    "    #print(most_common, num_most_common)\n",
    "    m2.append(most_common1)\n",
    "tf1 = pd.DataFrame(m2).to_csv('/media/ubuntu/HDD_P2/MDD/final2/globus_GMM+kMeans.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3a60b7-4deb-4959-8098-d2dbd91b53da",
   "metadata": {},
   "source": [
    "# prediction counter for Bids_mdd data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ce3f0c-8103-4246-9094-fb7f45d05c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#counting final label for each subject based on maximum number of occurence \n",
    "from collections import Counter\n",
    "m3 =[]\n",
    "sum = 0\n",
    "for i1 in range(0,40):\n",
    "    #print('i=',i1)\n",
    "    l2= f1[i1]\n",
    "    sum = sum+l2\n",
    "    #print('sum=', sum)\n",
    "    #print('l2=', l2)\n",
    "    #print('i=',f1[i],'i+1=',f1[i+1])\n",
    "    l1 = train_label[sum : sum+f1[i1+1]]\n",
    "    print(range(sum, sum+f1[i1+1]))\n",
    "    #print(l2+((((((f1+1)[i]*i+1)-(i))-1))))\n",
    "    #l1 = pred2[(((((f1+1)[i])*i+1)-(i))-1) :((f1[i])*(i+1)+1)-1]\n",
    "    #print((range(l2+((((f1+1)[i]*i+1)-(i))-1),((f1[i])*(i+2)+1)-1)))\n",
    "    #print(l1.shape)\n",
    "    most_common3,num_most_common3 = Counter(l1).most_common(1)[0] # 4, 6 times\n",
    "    #print(most_common3, num_most_common3)\n",
    "    m3.append(most_common3)\n",
    "tf1 = pd.DataFrame(m3).to_csv('/media/ubuntu/HDD_P2/MDD/final2/bids_GMM+kMeans.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71894d9-d2cf-47bd-a004-81eaf130a927",
   "metadata": {},
   "outputs": [],
   "source": [
    "m3.count(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
